\subsection*{Historical Background}

Of what is the universe made? This question has intrigued human curiosity since the dawn of time. 
Today, we are confident that we do not know the complete answer to this question.
However, a lot of progress has been made with the aim of reducing the diversity of the physical phenomena 
observed around us to a limited number of constituents following fundamental principles.
Over two thousand years ago, the ancient Greeks  postulated that all is made of Earth, Air, Fire and Water.
%This view represents a compact yet incorrect description of the world.
Fast-forward to the end of the 19$^\text{th}$ century, Mendeleev and others made the astonishing remark that by organizing the 
relative atomic masses of chemical elements, elements with similar chemical properties followed a pattern.
The periodic table of elements was born. 
The predictive power of the periodic table led to the anticipation of new elements that were later discovered.
However, the table lacked compactness and needed a more fundamental underlying structure that could 
connect the different elements together. 

At the turn of the 20$^\text{th}$ century, several important discoveries established 
the existence of the atom and its constituents. 
The atom is formed by electrons bound via the electromagnetic force to a nucleus, where nearly all the mass resides.
The nucleus itself is formed from 
protons and neutrons that are ``glued'' together by the strong nuclear force (or strong force).
These elements formed the underlying substructure that explained qualitatively the systematic organization of the periodic table.
After 1913, quantum ideas were applied to the atom offering a quantitative description of the origin of structure in atoms and molecules, 
including the chemical elements and their properties.
The decades that followed refined our understanding of the composition of matter through a series of experimental results.
By studying the collisions of protons, neutrons, and electrons in the 1950's and 1960's, a plethora of new particles were discovered 
which belonged to the same family as the proton and neutron, called \textit{hadrons}. % which interacted via the strong force
These particles could not all be elementary
\footnote{Elementary particles refer to  particles that cannot be decomposed into further constituents.}.
By invoking a similar argument that atoms were composite based on Mendeleev's table,
a new layer of structure was unfolded to reveal the existence of \textit{quarks} as basic constituents of all hadrons.
Six types of quarks were discovered over the years with the top quark discovered at Fermilab in Chicago, Illinois 
in 1995, being the most massive elementary particle
\cite{PhysRevLett.74.2626,Abachi:1995iq}.

The observation of the continuous energy spectra in the radioactive $\beta$ decay of nuclei 
led to the discover of neutrinos to remedy  the energy conservation law in the decay. 
Neutrinos are very light neutral particles that interact via the weak nuclear force 
responsible for radioactivity and nuclear fusion, the process that powers the stars.
Electrons and neutrinos had other relatives collectively called \textit{leptons}.
The quarks and leptons are referred to as \textit{fermions} and
have a half integer spin, an intrinsic property of elementary particles.
The strong, weak, and electromagnetic interactions are mediated by 
gluons, $W$ and $Z$ particles, and photons, respectively. These particles 
have an integer spin and are called \textit{bosons}. 
%Apart from gravity, all aspects of daily life can 
%be described in terms of these interactions.
%
The latest addition to the known elementary particles happened in 2012 
 with the discovery of 
a new boson, the Higgs boson, 
that allows the quarks and leptons and the $W$ and $Z$ bosons to 
acquire mass\cite{Aad:2012tfa,Chatrchyan:2012xdj}.

%A success story
\subsection*{The Standard Model}
The physics of elementary particles became the most ambitious and organized attempt to answer the question of what the universe is 
made out of. 
Through a mixture of both theoretical insight and experimental input, we now 
know that everything we see in our daily life is formed from quarks and leptons
that interact via the strong, weak, and electromagnetic forces.\footnote{ 
The fourth fundamental force of gravity is extremely weak and
only acts at the macroscopic scale.}
The forms of these forces are determined from basic principles of 
symmetry and invariance.
As a result, a theoretical framework was constructed to synthesize all these 
developments in a quantitative 
calculational tool that became known as the \textit{Standard Model of particle physics} (SM). 
The only inputs needed by the SM are the interaction strengths of the forces and quark and lepton masses to make 
very accurate predictions about the behavior of elementary particles.
%For example, the magnetic moment of the electron calculated in the SM is found to agree with the experimental measurement to nine(?) decimal places.
Over the past 30 years, the SM has been vigorously tested by many 
experiments and has been shown to accurately describe particle 
interactions at the highest energies produced in the laboratory.
 Yet, we know it is not the complete story. 

% the dark side
\subsection*{Limitations}
In 1933, an observation of the Coma Cluster by Fritz Zwicky suggested that the galaxies in the cluster were moving too fast to be explained 
by the luminous matter present\cite{1933AcHPh}. 
The same observation was repeated when looking at the rotation speeds of individual galaxies which 
suggested an invisble component of mass, dark matter. 
The experimental evidence established that dark matter is not made out of baryons
and is more abundant than ordinary matter.
For example, anisotropies in the cosmic microwave background, a radiation 
left over from the Big Bang, that were consistent with 
quantum fluctuations from an inflationary epoch \cite{Hu:2001bc,2009AIPC}, 
encoded details about the density of matter 
in the form of 
cosmological parameters as they traveled through space and time to reach 
our experiments.
The astonishing conclusion was that the universe has nearly five times 
as much dark matter as ordinary matter \cite{Bertone:2004pz}.

Supernovae surveys gave direct evidence for an accelerating universe
 \cite{Perlmutter:1998np},
% From the rate of expansion, we can play the scenario back in time and 
%deduce that the universe was compacted on itself some 14 billion years ago.
% the explosive erruption from that state is referred to as the Big Bang
a view that was cemented by the measurement of cosmological parameters
\cite{Adam:2015rua,Ade:2015xua}
which led to the startling discovery that most of the energy density of 
the universe is in the 
form of an unknown negative-pressure, called dark energy \cite{Scranton:2003in}.
There is an extensive program of experiments 
which will probe the dark energy. % and is beyond the scope of this work. 
%Instead, we turn to dark matter. 

Astrophysics and cosmology told us about 
the existence of dark matter and measured its density to a remarkable 
precision. Particle physics holds the hope to uncover what dark matter is.
In short, all experimental evidence is consistent with a universe 
constructed of 
\begin{itemize}
\item baryons (everyday matter): $\sim 5\%$ 
\item dark matter: $\sim 20\%$ 
\item dark energy: $\sim 75\%$ 
\item neutrinos, photons: a tiny fraction
\end{itemize}

Today, we are confronted by many puzzles related to our view of the universe.
Everything we know of, namely all the particles of the Standard Model, 
constitute only 5\% of the energy budget of the universe. 
The universe is also predominately composed of matter as opposed to 
anti-matter even though at the start of the universe, they were in equal 
amounts%, baryon assymetry
. The Standard Model describes the content of 
everyday matter 
and how it interacts but without telling us why it is that way.
Moreover, the Standard Model only describes these phenomena 
up to an energy scale of $\mathcal{O}\left(100\right)$ \GeV, called the weak scale.
Beyond this scale lies the realm of phenomena not described by the standard 
model that extend all the way to the Planck scale of 
$\mathcal{O}\left(10^{19}\right)$ \GeV. There is no mechanism to generate mass for 
neutrinos in the Standard Model. Last but not least, the Standard Model 
does not incorporate gravity, the fourth fundamental force.
The SM is unable to account for these observed 
features in the universe. 
Thus, there is a need for a theory beyond the Standard Model.

\subsection*{Supersymmetry}

One of the most prominent extensions of the Standard Model, 
that addresses many of the shortcomings mentioned above, is a theory based on 
a new symmetry, called supersymmetry.
This symmetry is between the matter particles, fermions, and particles whose
exchange mediates the forces, bosons. Our current description of the world
treats fermions and bosons differently. Supersymmetry puts forward the idea
that fermions and bosons can be treated in a fully symmetric way. 
In other words,
if we exchange fermions and bosons in the equations of the theory, the 
equations will still look the same. An immediate consequence of the theory
is that every Standard Model particle will have a ``superpartner,'' 
none of which have yet been discovered.
As a result, we can design experiments to search for these 
supersymmetric particles. The work presented in this dissertation is about the search for supersymmetric particles with a specific signature.
The many benefits of supersymmetry will be discussed later but here it is worth 
mentioning two important features of the theory: 
it unifies the three interactions, electromagnetic, strong, and weak forces,
at very high energies 
%it addresses the question 
%of why there is a huge gap between the Planck scale and the weak scale, 
and it provides a dark matter candidate particle. Now that we understand 
what we are trying to do, it is time to address the question of how to do it.

\subsection*{Experimental techniques}

The human eye can resolve pieces of dust up to $10^{-5}$ m.
The subatomic distances we are interested in probing range
from  $10^{-15}$ m, the size of a proton, 
down to $10^{-18}$ m, the experimental limit to the maximum size of a quark.
Instruments are needed to extend our senses to 
probe these very small scales.
For instance, light microscopes can reveal the structure of things down to 
$10^{-6}$ m, the scale of bacteria and molecules. 
A special type of microscope is needed to probe smaller distances, 
 a particle accelerator.
The basic idea is that in order to see an object, a wave must scatter off 
this object and must have a wavelength smaller than the object being 
probed.
Since particles have a wavelike character, they can be used to 
probe ever shorter distances according to
\[
E = \frac{hc}{\lambda}
\]
where $E$ is the energy of the particle, $\lambda$ is its wavelength, 
and $hc \sim 10^{-6}$ eVm. As a result, 
the higher the speed of the particles, the greater their 
energy and momentum and the shorter their associated wavelength.
Modern accelerators can generate energy in the \TeV~scale and thus probe 
a distance of $10^{-18}$ m.
All the development that we have made 
describes phenomena happening at distances larger than about $10^{-18}$ m.
Thus, it is possible that electrons and quarks have some structure which is 
smaller than what we can resolve in experiment. For this reason, we 
currently consider them as not having any deeper structure, i.e. they are 
called pointlike objects. 

Over the last century, beams of particles were used to study the 
composition of matter.
Initially, beams originated from phenomena that were 
already naturally occurring, such as alpha and beta particles coming from radioactive 
decays and cosmic rays.
Some cosmic rays are much more energetic than what we can produce 
in the laboratory today, however, they occur at random and 
with a low intensity. Instead, 
high energy particle accelerators were used to deliver high intensity beams of 
electrons, protons, and other particles under controlled conditions.
For this reason, particle physics is also known as high energy physics.
By colliding two sufficiently energetic particles,  new particles will be created 
according to Einstein's equation $E = mc^2$ (or more generally $E = \sqrt{\left(mc^2\right)^2+\left(pc\right)^2}$),
where energy can be exchanged for mass, and vice versa, the exchange rate being $c^2$, the square of the 
speed of light. For example, an electron has a mass of 0.5 \MeV~ 
and can only be created in an electron-positron pair, thus 
1 \MeV~of energy is needed for an electron--positron pair to be produced at rest.
Energies in the \TeV~ range were present about a billionth of a second after the Big Bang.
In other words, by colliding high energy particles, it is possible to 
recreate momentarily conditions similar to those of the universe when it 
was newly born.
At such energies, particles and antiparticles were created, including 
exotic forms no longer common today.
Most of the particles generated in these collisions are extremely short lived 
with lifetimes less than $10^{-20}$ seconds, 
producing radiation and decaying to stable particles, such as electrons and quarks, that make up most of what we see today.
One of the exotic forms of matter that may exist is supersymmetry.
The search for evidence for supersymmetric particles using data collected at a
high energy particle accelerator is the subject of this dissertation.

The Large Hadron Collider (LHC) 
is the world's most energetic 
particle accelerator and the pinnacle of colliding beam technology.
Is it located at CERN, 
the European Laboratory for Particle Physics
\footnote{
The acronym comes from French ``Conseil Européen pour la Recherche Nucléaire''
which was established to do fundamental physics research.
In 1952, this research concentrated on understanding the atom and its nucleus, hence the word ``nuclear''.
Today, our knowledge goes deeper than the nucleus which motivates the modified name.
}, near Geneva, Switzerland.
The LHC accelerates counter rotating beams of protons 
to 99.9999991\% the speed of light in a 27 km ring reaching an energy of 
6.5 \TeV~per beam. 
Magnets, cooled 
by the largest cryogenic system in the world to 1.9 K (-271.3 $^{\circ}{\textrm C}$), that keep the 
protons on track and bring the counter-rotating needle-like
beams 
%of few micrometers in diameter 
to meet head on 40 million times per second. 
The debris of each collision fly off in all directions, 
briefly producing less common exotic forms of matter
captured by large particle detectors in the form of ``snapshots'' of these collisions, called events.
The teams of scientists analyze these events to identify the different particles that were produced 
and reconstruct the full collision process.
With this information, it is possible to make precision measurements of rare Standard Model processes, like 
the production of the Higgs boson, or search for physics beyond the Standard Model, like evidence 
for supersymmetry.
ATLAS is one of the general-purpose particle detectors at the LHC that supplied the events 
analyzed in this dissertation to search for supersymmetry. 
The ATLAS detector is the largest-volume particle detector ever built --
the size of a seven-story building 46 meters high and 26 meters in diameter, 
weight 7000 tonnes, %has about 3000 km of cable, 
and able to measure particle trajectories down to 0.01 meters.
% The collaboration has nearly 3000 scientific authors from 182 institutions
% in 38 countries (June 2017).
Bunches of protons pass through each other at the heart of 
the ATLAS detector 40 million times per second.
Each time they cross there are on average 25 proton-proton collisions, leading to 
about a billion proton collisions per second. 
The data generated in these collisions amounts to about 60 terabytes per second, 
an amount far beyond what is technologically possible to store.
In fact, the processes of interest are extremely rare.
For example, the Higgs boson is produced once in 20 million million collisions.
In more practical terms, a Higgs boson might appear once a day during the LHC operations.
ATLAS has a big computational challenge to recognize this one Higgs event and record it to tape 
out of 35 million million other collisions each day.
The topic of this dissertation is to search for supersymmetric particles that 
are even rarer and thus more challenging to look for. 

This dissertation will give a detailed explanation on how we searched for supersymmetric 
particles using the ATLAS detector.
First, the motivation behind the work will begin with an overview of the 
Standard Model of particle physics and supersymmetry in 
Chapter~\ref{chap:theory} followed by the design of the ATLAS detector 
at the LHC in Chapter~\ref{chap:exp}.
The Region of Interest Builder that processes every event recorded by ATLAS 
is covered in Chapter~\ref{chap:roib}.
The detailed description of the search starts in Chapter~\ref{chap:strategy}
covering the basic analysis strategy and the supersymmetric models considered.
The most challenging part of the analysis is the estimation of Standard Model and 
detector backgrounds with novel techniques developed by the author and covered in 
Chapters~\ref{chap:fake} and ~\ref{chap:bkg}. The statistical 
methodology and interpretation of the results is presented in 
Chapters~\ref{chap:stat} and ~\ref{chap:res}.
This analysis represents an important search for supersymmetric particles 
with the early data-set collected by ATLAS at a new center of mass energy of 13 \TeV.
The strength of the search lies in exploring regions of the parameter space 
with a small mass difference between the supersymmetric particles, regions
that are difficult to probe with other searches for unknown physics.
