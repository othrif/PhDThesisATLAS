In order to interpret the LHC data, it is essential to compare the observations to the expected outcomes from a physical model, typically the Standard Model and a SUSY scenario.
The event simulation starts from a proton-proton ($pp$) collision leading to the process of interest all the way to the expected detector response. 
These steps are the following:

\begin{itemize} 
\item Event generation: The process of interest $pp \to X$ is generated by relying on random sampling using Monte Carlo (MC) techniques, which repeatedly draw samples that represent 
  a possible outcome of a given process. The processes are generated using a software package, \textsc{Madgraph} ~\cite{Alwall:2014hca} for example, which calculates the matrix element 
  for each process to some order in QCD. Generators start from partons\footnote{Partons refer to all the particles that can exist inside 
the proton: quarks, anti-quarks, and gluons.} of a $pp$ collision, using parton distribution functions, and calculate the processes up to leading order or next-to-leading order. 
  For processes where the cross-section at a higher order is non-negligible a factor called the $k$-factor\footnote{The $k$-factor 
    describes the difference between the leading order cross section and higher order cross sections.} 
  is applied to the expected cross-section. 
  The partons from the hard interaction are colored and radiate gluons described by a parton showering software, as \textsc{Pythia} ~\cite{Sjostrand:2007gs}. The generators used in the analysis 
  are given in Section~\ref{sec:strategy.samples}. 
  The raw output of such generators 
  is an input to the next steps of simulation. They can also be used to perform generator level studies, also called ``truth level'', undergoing minimal processing to evaluate the 
  sensitivity and  acceptance of the analysis (for example Table~\ref{tab:strategy.cut}).
\item Detector simulation: The event generator gives particle momenta at the hadron level which are then processed by \textsc{Geant4}~\cite{Agostinelli:2002hh}, which simulates
  the propagation of particles through the different materials comprising the detector. The simulation includes  the best knowledge of the detector 
geometry, material budget and modeling of the particle interactions.
The full simulation of the detector is a slow process. For many applications, such as the generation 
  of SUSY signals, it is faster to use a parametrized response of the calorimeters~\cite{ATL-PHYS-PUB-2010-013}.
\item Digitisation: The detector simulation records the interaction of particles with the different components of the detector in the form of hits and energy deposits in the detector.
  The latter are used as inputs to emulate the response of the readout electronics of the detector. The output from this step is identical to the data recorded by the detector.
\item Reconstruction: At this stage, both the recorded events by the detector and simulated events are used to identify objects associated with fundamental particles, namely 
  electrons, muons, photons, and jets. The energy deposits not matched to physics objects are collected into a ``soft terms'' category used in the computation of the missing transverse momentum.  
\end{itemize} 

There are two other important elements of the simulation that are less understood. The simulated MC samples must handle the underlying event, which is the remainder of the non-hard scattered partons of 
the original interacting protons. Also, the MC generators must simulate the interactions between the other particles in the beam crossing, also referred to as pileup. In practice, 
the MC is generated with an expected pileup profile which is later corrected based on the observed pileup profile.


The final physics objects from the reconstruction step do not reflect all the knowledge we have about the detector. For instance, the energies of the objects must be calibrated
or certain parts of the detector may not always be working with the desired specification. 
 Once the reconstructed objects are defined and calibrated, the data is ready for analysis. Typically, the size of the dataset is reduced from petabytes to a size of few terabytes
  by only selecting the objects of interest in the analysis. For instance, this analysis requires at least two leptons applied to the samples used which significantly reduce the sample size.
